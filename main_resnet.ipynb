{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_resnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1230038/googleColaboScrapbook/blob/master/main_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOrHqhXs7klL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Original: https://github.com/a-martyn/resnet\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5p-22gY8STN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd\n",
        "!cp -r /content/drive/My\\ Drive/resnet_files/* ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pys6DjwK7N5p",
        "colab_type": "text"
      },
      "source": [
        "# Deep Residual Learning for Image Recognition: CIFAR-10 \n",
        "\n",
        "This notebook provides a PyTorch implementation of *Deep Residual Learning for Image Recogniton* by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Han Sun which achived state of the art in 2015 by winning the ILSVRC and COCO challenges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvSTyQhB7N5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8VpWk1M7N5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from resnet import ResNet\n",
        "from data_loader import get_data_loaders, plot_images\n",
        "from utils import calculate_normalisation_params\n",
        "from train import train\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EfCUPeq7N5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GLOBALS\n",
        "# -----------------------\n",
        "\n",
        "data_dir = 'data/cifar10'\n",
        "batch_size = 128\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1s62ZOn7N5y",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation: Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DMEkcdXh7N5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VISUALISE\n",
        "# -----------------------\n",
        "\n",
        "# Load data without normalisations\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    # 4 pixels are padded on each side, \n",
        "    transforms.Pad(4),\n",
        "    # a 32×32 crop is randomly sampled from the \n",
        "    # padded image or its horizontal flip.\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    # For testing, we only evaluate the single \n",
        "    # view of the original 32×32 image.\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_loader, test_loader = get_data_loaders(data_dir,\n",
        "                                             batch_size,\n",
        "                                             train_transform,\n",
        "                                             test_transform,\n",
        "                                             shuffle=True,\n",
        "                                             num_workers=4,\n",
        "                                             pin_memory=True)\n",
        "\n",
        "\n",
        "# Training images\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = data_iter.next()\n",
        "X = images.numpy().transpose([0, 2, 3, 1])\n",
        "plot_images(X, labels)\n",
        "print(images.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1pdhvxD7N53",
        "colab_type": "text"
      },
      "source": [
        "The training set consists of 50,000 32x32 pixel images. These are padded with 4 pixels on each side, before a crop is randomly sampled from the padded image or its horziontal flip. \n",
        "\n",
        "The authors cite C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-supervised nets. arXiv:1409.5185, 2014. This paper suggests that 'corner cropping' should be used which could be interpretted one of two ways:\n",
        "\n",
        "1. There are five possible crops for each image, centre and then four further crops with the crop placed tightly in each corner. This equates to `torchvision.transforms.FiveCrop()` in PyTorch.\n",
        "2. The crop position is randomly selected from any possible position within the bounds of the padded image. Equivalent to `torchvision.transforms.RandomCrop()` in PyTorch.\n",
        "\n",
        "The intended implementation seem ambiguous to me, so I implemented the latter here largely for simplicity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaStCP1S7N54",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation: Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgCat6De7N55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test images\n",
        "data_iter = iter(test_loader)\n",
        "images, labels = data_iter.next()\n",
        "X = images.numpy().transpose([0, 2, 3, 1])\n",
        "plot_images(X, labels)\n",
        "print(images.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn-MJ2sx7N58",
        "colab_type": "text"
      },
      "source": [
        "**There** are 10,000 images in the test set. These are not transformed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0pll1zu7N59",
        "colab_type": "text"
      },
      "source": [
        "## Calculate normalisation parameters\n",
        "\n",
        "The authors don't explicitly state if the images are normalised prior to training, however the paper referenced on preprocessing (C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-supervised nets. arXiv:1409.5185, 2014) notes that normalisation is used. \n",
        "\n",
        "I was unsure if the mean and standard deviation of the dataset should be calculated before or after augmentation for this purpose. It is also unclear if these calcualations should be done across the training and test set combined, or only the training set.\n",
        "\n",
        "Her I've used the pre-transform training and test set combined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ47z8oP7N5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NORMALIZATION\n",
        "# -----------------------\n",
        "# Calculate the mean and standard deviation of each channel\n",
        "# for all observations in training and test datasets. The\n",
        "# results can then be used for normalisation\n",
        "\n",
        "# Load data without transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    # 4 pixels are padded on each side, \n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    # For testing, we only evaluate the single \n",
        "    # view of the original 32×32 image.\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_loader, test_loader = get_data_loaders(data_dir,\n",
        "                                             batch_size,\n",
        "                                             train_transform,\n",
        "                                             test_transform,\n",
        "                                             shuffle=True,\n",
        "                                             num_workers=4,\n",
        "                                             pin_memory=True)\n",
        "\n",
        "\n",
        "# Takes a while to run so I've hardcoded results below\n",
        "\n",
        "means, stds = calculate_normalisation_params(train_loader, test_loader)\n",
        "print(f'means: {means}')\n",
        "print(f'stds: {stds}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIYFlxdD7N6B",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "Here we finalise data augmentation and normalisation parameters ahead of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLW5ts37N6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SET FINAL TRANSFORMS WITH NORMALISATION\n",
        "\n",
        "# [x] simple data augmentation in [24]\n",
        "# [x] 4 pixels are padded on each side, \n",
        "# [x] and a 32×32 crop is randomly sampled from the padded image or its horizontal flip.\n",
        "# [x] For testing, we only evaluate the single view of the original 32×32 image.\n",
        "\n",
        "\n",
        "# Normalisation parameters fo CIFAR10\n",
        "means = [0.4918687901200927, 0.49185976472299225, 0.4918583862227116]\n",
        "stds  = [0.24697121702736, 0.24696766978537033, 0.2469719877121087]\n",
        "\n",
        "normalize = transforms.Normalize(\n",
        "    mean=means,\n",
        "    std=stds,\n",
        ")\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    # 4 pixels are padded on each side, \n",
        "    transforms.Pad(4),\n",
        "    # a 32×32 crop is randomly sampled from the \n",
        "    # padded image or its horizontal flip.\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    # For testing, we only evaluate the single \n",
        "    # view of the original 32×32 image.\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTUPW6JE7N6F",
        "colab_type": "text"
      },
      "source": [
        "Notes:\n",
        "\n",
        "- Note: downsampling with stride 2 results in uneven convolution, e.g. W_out is 16.5. Why didn't the authors use Maxpooling, would have been neater."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPOwf9W77N6G",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn0VqoTL7N6G",
        "colab_type": "text"
      },
      "source": [
        "The authors report their results in term of training iterations. I've assumed an iteration here means a forwards and backwards pass of a batch of 128 observations through the entire network.\n",
        "\n",
        "It seems more convenient to work in epochs in PyTorch, so here I calculate the integer number of iterations per epoch for later conversions. The results is 391 iterations per epoch. An epoch is a forwards and backwards pass for all observations in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSxdCLCd7N6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many iterations in an epoch?\n",
        "\n",
        "iterations = 0\n",
        "for i, data in enumerate(train_loader, 0):\n",
        "    iterations +=1\n",
        "print(iterations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-thj1XFK7N6K",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters\n",
        "\n",
        "Set the hyperparameters as described by the authors, with iterations converted to epochs where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEUJuptO7N6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAINING PARAMETERS\n",
        "# -------------------------\n",
        "\n",
        "# Authors cite 64k iterations\n",
        "# 64000/391 = 164\n",
        "epochs = 164\n",
        "\n",
        "# OPTIMISER PARAMETERS\n",
        "lr = 0.1 # authors cite 0.1\n",
        "momentum = 0.9\n",
        "weight_decay = 0.0001 \n",
        "\n",
        "# LEARNING RATE ADJUSTMENT\n",
        "# Reduce learning rate at iterations \n",
        "# 32k and 48k. Convert to epochs:\n",
        "# e.g. iterations / (n_observations/batch_size) \n",
        "# 32000/391 = 82\n",
        "# 48000/391 = 123\n",
        "milestones = [82, 123]\n",
        "# Divide learning rate by 10 at each milestone\n",
        "gamma = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk1NBlWo7N6P",
        "colab_type": "text"
      },
      "source": [
        "### Plain nets\n",
        "\n",
        "Run training loop for **'plain' convolutional networks** without shortcuts. Here we run for n={3, 5, 7,9} corresponding to networks of depth 20, 32, 44 and 64 layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRGCUYcS7N6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAIN PLAIN NETs\n",
        "\n",
        "# n determines network size as described in paper\n",
        "# where total number of layers is (6*n)+2\n",
        "ns = [3, 5, 7, 9]\n",
        "\n",
        "# Train plainnets\n",
        "for n in ns:\n",
        "    print(f'MODEL SIZE: n={n}')\n",
        "    # Reload data\n",
        "    train_loader, test_loader = get_data_loaders(data_dir,\n",
        "                                                 batch_size,\n",
        "                                                 train_transform,\n",
        "                                                 test_transform,\n",
        "                                                 shuffle=True,\n",
        "                                                 num_workers=4,\n",
        "                                                 pin_memory=True)\n",
        "    \n",
        "    model = torch.nn.DataParallel(ResNet(n, shortcuts=False)) # modified from original\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "    results_file = f'results/plainnet{6*n+2}.csv'\n",
        "    model_file = f'pretrained/plainnet{6*n+2}.pt'\n",
        "    train(model, epochs, train_loader, test_loader, criterion, \n",
        "          optimizer, results_file, scheduler=scheduler, MODEL_PATH=model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA7p4vyJ7N6T",
        "colab_type": "text"
      },
      "source": [
        "### Residual nets\n",
        "\n",
        "Run training loop for residual networks with shortcuts. Here we run for n={3, 5, 7,9} corresponding to networks of depth 20, 32, 44 and 64 layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOSyNoLU7N6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAIN RESNETs\n",
        "\n",
        "# n determines network size as described in paper\n",
        "ns = [3, 5, 7, 9]\n",
        "\n",
        "# Train resnets\n",
        "for n in ns:\n",
        "    print(f'MODEL SIZE: n={n}')\n",
        "    # Reload data\n",
        "    train_loader, test_loader = get_data_loaders(data_dir,\n",
        "                                                 batch_size,\n",
        "                                                 train_transform,\n",
        "                                                 test_transform,\n",
        "                                                 shuffle=True,\n",
        "                                                 num_workers=4,\n",
        "                                                 pin_memory=True)\n",
        "    \n",
        "    model = torch.nn.DataParallel(ResNet(n, shortcuts=True)) # modified from original\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "    results_file = f'results/resnet{6*n+2}.csv'\n",
        "    model_file = f'pretrained/resnet{6*n+2}.pt'\n",
        "    train(model, epochs, train_loader, test_loader, criterion, \n",
        "          optimizer, results_file, scheduler=scheduler, MODEL_PATH=model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDzGxp2a7N6W",
        "colab_type": "text"
      },
      "source": [
        "### Check model\n",
        "\n",
        "Table 6. in the paper reports that the ResNet model with 20 layers should have 0.27 million trainable parameters. We reproduce that here.\n",
        "\n",
        "It is noted elsewhere that the plain net should have the same number of parameters for the CIFAR10 experiment. This is because an identity mapping is used for the residual shortcuts, as opposed to a convolutional layer, an so there are no additional trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LxfW1zP77N6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Parameter count: {sum([p.numel() for p in model.parameters()])}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HJ372Z47N6a",
        "colab_type": "text"
      },
      "source": [
        "The PyTorch model architecture for a 20 layer model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Rwg3zpYX7N6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rugs4Nun7N6g",
        "colab_type": "text"
      },
      "source": [
        "## Analysis\n",
        "\n",
        "### Plain net\n",
        "\n",
        "First we compare the performance of 'plain' convolutional networks of various depths. This corresponds to the left-hand axis of Figure. 6 in the paper. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "JEVMNgb27N6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ns  = [3, 5, 7, 9]\n",
        "clr = ['y', 'c', 'g', 'r']\n",
        "\n",
        "\n",
        "plainnet_dfs = [pd.read_csv(f'results/plainnet{6*n+2}.csv') for n in ns]\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "plt.axis([0, 164, 0, 20])\n",
        "\n",
        "\n",
        "for i in range(len(ns)):\n",
        "    plt.plot(plainnet_dfs[i]['epoch'], plainnet_dfs[i]['train_err']*100, f'{clr[i]}--',\n",
        "             label=f'plain-{6*ns[i]+2} train')\n",
        "    plt.plot(plainnet_dfs[i]['epoch'], plainnet_dfs[i]['test_err']*100, f'{clr[i]}',\n",
        "             label=f'plain-{6*ns[i]+2} test')\n",
        "\n",
        "plt.title('Comparison of four plain convolutional networks with 20, 32, 44 and 56 layers.')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('error (%)')\n",
        "plt.axhline(10, color='black', alpha=0.5, dashes=(10., 10.))\n",
        "plt.axhline(5, color='black', alpha=0.5, dashes=(10., 10.));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avfoDfOR7N6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bests = [df[df['test_err'] == df['test_err'].min()].iloc[0] for df in plainnet_dfs]\n",
        "bests_df= pd.concat(bests, axis=1).T\n",
        "bests_df['model'] = ['PlainNet20', 'PlainNet32', 'PlainNet44', 'PlainNet56']\n",
        "display(bests_df[['model', 'test_err']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-sXcI8F7N6q",
        "colab_type": "text"
      },
      "source": [
        "### Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MflgjT87N6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ns  = [3, 5, 7, 9]\n",
        "clr = ['y', 'c', 'g', 'r']\n",
        "\n",
        "resnet_dfs = [pd.read_csv(f'results/resnet{6*n+2}.csv') for n in ns]\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "plt.axis([0, 164, 0, 20])\n",
        "\n",
        "\n",
        "for i in range(len(ns)):\n",
        "    plt.plot(resnet_dfs[i]['epoch'], resnet_dfs[i]['train_err']*100, f'{clr[i]}--',\n",
        "             label=f'ResNet-{6*ns[i]+2} train')\n",
        "    plt.plot(resnet_dfs[i]['epoch'], resnet_dfs[i]['test_err']*100, f'{clr[i]}',\n",
        "             label=f'ResNet-{6*ns[i]+2} test')\n",
        "\n",
        "plt.title('Comparison of four residual networks with 20, 32, 44 and 56 layers.')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('error (%)')\n",
        "plt.axhline(10, color='black', alpha=0.5, dashes=(10., 10.))\n",
        "plt.axhline(5, color='black', alpha=0.5, dashes=(10., 10.));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO-3OP3T7N6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bests = [df[df['test_err'] == df['test_err'].min()].iloc[0] for df in resnet_dfs]\n",
        "bests_df= pd.concat(bests, axis=1).T\n",
        "bests_df['model'] = ['ResNet20', 'ResNet32', 'ResNet44', 'ResNet56']\n",
        "display(bests_df[['model', 'test_err']])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14nENvId7N6x",
        "colab_type": "text"
      },
      "source": [
        "## Side-by-side plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ2T9VVv7N6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ns  = [3, 5, 7, 9]\n",
        "clr = ['y', 'c', 'g', 'r']\n",
        "\n",
        "\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(25, 7))\n",
        "\n",
        "plainnet_dfs = [pd.read_csv(f'results/plainnet{6*n+2}.csv') for n in ns]\n",
        "resnet_dfs = [pd.read_csv(f'results/resnet{6*n+2}.csv') for n in ns]\n",
        "\n",
        "def plot_results(dfs, ax, title):\n",
        "    ax.axis([0, 164, 0, 20])\n",
        "    \n",
        "    for i in range(len(ns)):\n",
        "        ax.plot(dfs[i]['epoch'], dfs[i]['train_err']*100, f'{clr[i]}--',\n",
        "                 label=f'plain-{6*ns[i]+2} train')\n",
        "        ax.plot(dfs[i]['epoch'], dfs[i]['test_err']*100, f'{clr[i]}',\n",
        "                 label=f'plain-{6*ns[i]+2} test')\n",
        "    \n",
        "    ax.set_title(title)\n",
        "    ax.legend(loc='bottom left')\n",
        "    ax.set_xlabel('epochs')\n",
        "    ax.set_ylabel('error (%)')\n",
        "    ax.axhline(10, color='black', alpha=0.5, dashes=(10., 10.))\n",
        "    ax.axhline(5, color='black', alpha=0.5, dashes=(10., 10.));\n",
        "    \n",
        "plot_results(plainnet_dfs, ax1, 'Plain Networks')\n",
        "plot_results(resnet_dfs, ax2, 'Residual Networks')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvLONo5-7N60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}